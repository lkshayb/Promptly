/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as SarvamAI from "../../../../index";
/**
 * @example
 *     {
 *         messages: [{
 *                 role: "assistant",
 *                 content: "content"
 *             }],
 *         model: "sarvam-m"
 *     }
 */
export interface ChatCompletionsRequest {
    /** A list of messages comprising the conversation so far. */
    messages: SarvamAI.ChatCompletionRequestMessage[];
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     * We generally recommend altering this or `top_p` but not both.
     */
    temperature?: number;
    /**
     * An alternative to sampling with temperature, called nucleus sampling,
     * where the model considers the results of the tokens with top_p probability
     * mass. So 0.1 means only the tokens comprising the top 10% probability mass
     * are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     */
    top_p?: number;
    reasoning_effort?: SarvamAI.ReasoningEffort;
    /** The maximum number of tokens that can be generated in the chat completion. */
    max_tokens?: number;
    /**
     * If set to true, the model response data will be streamed to the client
     * as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
     */
    stream?: boolean;
    stop?: SarvamAI.StopConfiguration;
    /** How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs. */
    n?: number;
    /**
     * This feature is in Beta.
     * If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
     * Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
     */
    seed?: number;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on
     * their existing frequency in the text so far, decreasing the model's
     * likelihood to repeat the same line verbatim.
     */
    frequency_penalty?: number;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on
     * whether they appear in the text so far, increasing the model's likelihood
     * to talk about new topics.
     */
    presence_penalty?: number;
    /** If this parameter is enabled, then the model uses a RAG based approach to retrieve relevant chunks from Wikipedia and uses them to answer the question. This is particularly useful for queries seeking factual information. */
    wiki_grounding?: boolean;
}
